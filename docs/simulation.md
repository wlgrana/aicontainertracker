
# Simulation Pipeline Documentation

This document describes the 5-step autonomous simulation pipeline for the Shipment Tracker, its logging infrastructure, and known issues.

## Pipeline Steps

The simulation automates the "Self-Improving Ingestion Engine" workflow:

1.  **Archivist (Step 1)**
    *   **Action**: Reads the uploaded Excel file (`Horizon Tracking Report.xlsx`).
    *   **Logic**: Uses AI (or heuristics) to identify the header row and cleans the data.
    *   **Output**: Stores raw rows in the `RawRow` database table.

2.  **Translator (Step 2)**
    *   **Action**: Maps user headers to the canonical schema (e.g., "PO Number" -> `customer_po`).
    *   **Logic**: Uses `Translator` agent (OpenAI) with a fallback to "Heuristic Matching". **Includes Robust Date Conversion**: Automatically detects and converts Excel serial dates (e.g., `45719`) to ISO format (`2025-03-03T00...`).
    *   **Output**: Generates a `temp_translation.json` artifact defining the schema mapping.

3.  **Auditor (Step 3)**
    *   **Action**: Pre-validates the mapping before committing to the database.
    *   **Logic**: Runs a **"Fast Check" (1-Row Sample)** to validate the mapping logic instantly using the `Auditor` agent. Checks for data quality issues or unmapped fields without stalling the pipeline.
    *   **Output**: Updates the mapping artifact with "Auto-Patches" if it finds obvious missing fields (e.g., `Remaks` -> `metadata.remarks`). These patches are immediately available to the Importer and Learner.

4.  **Importer (Step 4)**
    *   **Action**: bulk transforms and saves all data.
    *   **Logic**: Uses **Optimized Batch Persistence** (Chunk Size: 50). Applies the mapping artifact to ALL rows. **Merges Enriched Data**: Automatically runs the Enricher and persists derived fields (`serviceType`, `finalDestination`) into the canonical record to prevent data gaps.
    *   **Output**: Live database records. Displays a **1-Row Verification Sample** in the logs.

5.  **Learner (Step 5)**
    *   **Action**: Learns from unmapped data to improve future runs.
    *   **Action**: Learns from unmapped data to improve future runs.
    *   **Logic**: 
        1.  Scans the imported containers for "Unmapped Fields" (stored in metadata) and uses the `ImprovementAnalyzer` AI.
        2.  **Reads the `temp_translation.json` artifact** to capture successful "Auto-Patches" generated by the Auditor in Step 3.
    *   **Output**: Updates `agents/dictionaries/business_units.yml` or `container_ontology.yml` automatically, ensuring the system learns from its own corrections.

## Logging Infrastructure

To verify runs and troubleshoot issues without relying on the UI console, we have implemented a robust file-based logging system.

*   **Location**: `logs/simulation.log` (Project Root)
*   **Behavior**:
    *   Real-time logs are appended here during every simulation step.
    *   **Auto-Archiving**: When a new simulation starts, the previous log is renamed to `logs/simulation_<timestamp>.log`.
*   **Usage**:
    *   Tail this file to see Agent thoughts, API responses, and specific errors.
    *   Example: `cat logs/simulation.log`

## Current Issues & Troubleshooting

### 1. Auditor "Stuck" State (Resolved)
**Symptoms**: The simulation hangs during Step 3 (Auditor). The logs show 2-3 containers processed, then silence. The test script times out after 60 seconds.
**Cause**: The `Auditor` agent makes sequential, heavy AI calls. Long prompts or slow API responses caused timeouts.
**Resolution**:
*   **Increased Timeout**: AI call timeout increased to 30 seconds.
*   **Retry Logic**: Implemented exponential backoff (up to 2 retries) to handle transient failures.
*   **Input Validation**: Fixed a bug where `rawData.raw.originalRow` was undefined, creating an immediate failure loop.
*   The simulation now successfully completes even if the auditor encounters transient network issues.

### 2. "Node.exe" Popups (Resolved)
**Symptoms**: Infinite loop of command prompt windows opening/closing.
**Cause**: Race condition between Frontend `status` polling and Backend `spawn`.
**Fix**: `app/api/simulation/control/route.ts` now updates status *synchronously* before spawning child processes. Step scripts use `shell: false` and `windowsHide: true`.

### 3. Step 4 "Null" Data (Resolved)
**Symptoms**: Imported containers showed `null` for all fields.
**Cause**: `transformRow` utility expected Array input but received Key-Value Objects.
**Fix**: Updated `lib/transformation-engine.ts` to handle both formats.

<<<<<<< Updated upstream
## Environment Compatibility

### Vercel / Serverless Deployments
The **Simulation Engine** (`/simulation`) is architected for **local demonstration only**. It relies on:
1.  **Persistent Filesystem**: To store state (`simulation_status.json`), log archives (`logs/`), and temporary artifacts (`temp_translation.json`) between steps.
2.  **Child Processes**: To spawn long-running TypeScript scripts (`step1_...ts`) that execute the heavy AI agent logic independently of the HTTP request cycle.

These features are **incompatible** with Vercel's serverless environment (Read-Only filesystem, no background process persistence).

**Behavior on Vercel**:
*   The "Start Simulation" and "Upload File" endpoints (`api/simulation/control`, `api/upload`) are **protected**.
*   They will return a `200 OK` with `success: false` and a descriptive message.
*   The UI will display an alert: *"Simulation Engine is meant for local demonstration only..."*
*   **Operational Dashboard**: The rest of the application (Dashboard, Container Details, Search) functions normally on Vercel using the production database.
=======
### 4. "Left Over" Column Loss (Resolved)
**Symptoms**: Unmapped columns (like "Booking Date" or "Notes") were silently dropped during import and checking "meta" revealed nothing.
**Cause**: The `transformRow` utility attempted to access unmapped fields using numeric array indices (`rawData[i]`) even when the input was a Key-Value Object (JSON), resulting in `undefined` values.
**Fix**: Updated `lib/transformation-engine.ts` to detect input type (Array vs Object) and correctly access properties by key. This ensures **Zero Data Loss**â€”all unmapped fields are now preserved in `container.meta`, allowing the **Learner** (Step 5) to see and learn from them.

### 5. Excell Serial Dates (Resolved)
**Symptoms**: Dates appeared as integers (e.g., `45719`) or were invalid in the database.
**Cause**: Excel stores dates as serial numbers. The system was treating them as raw strings.
**Fix**: Implemented `lib/date-utils.ts` and integrated it into both **Translator** (Step 2) and **Persistence** (Step 4) to automatically detect and convert these values to ISO Dates.

### 6. Enricher Data Loss (Resolved)
**Symptoms**: Enriched fields (Service Type, Final Destination) appeared in logs but were NULL in the database.
**Cause**: The persistence layer calculated the `Container` object *before* running the Enricher, so the enriched values were calculated but never merged into the create/update payload.
**Fix**: Updated `lib/persistence.ts` to explicitly merge `enrichmentMap` results into the `Container` payload before the crucial `prisma.upsert` call. Enriched statuses are also mapped to valid enum values (e.g., `IN_TRANSIT` -> `DEP`).
>>>>>>> Stashed changes
