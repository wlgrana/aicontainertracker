
# Simulation Pipeline Documentation

This document describes the 5-step autonomous simulation pipeline for the Shipment Tracker, its logging infrastructure, and known issues.

## Pipeline Steps

The simulation automates the "Self-Improving Ingestion Engine" workflow:

1.  **Archivist (Step 1)**
    *   **Action**: Reads the uploaded Excel file (`Horizon Tracking Report.xlsx`).
    *   **Logic**: Uses AI (or heuristics) to identify the header row and cleans the data.
    *   **Output**: Stores raw rows in the `RawRow` database table.

2.  **Translator (Step 2)**
    *   **Action**: Maps user headers to the canonical schema (e.g., "PO Number" -> `customer_po`).
    *   **Logic**: Uses `Translator` agent (OpenAI) with a fallback to "Heuristic Matching" if the AI times out.
    *   **Output**: Generates a `temp_translation.json` artifact defining the schema mapping.

3.  **Auditor (Step 3)**
    *   **Action**: Pre-validates the mapping before committing to the database.
    *   **Logic**: Simulates the import for a small sample (3-5 rows) and runs the `Auditor` agent to check for data quality issues or unmapped fields.
    *   **Output**: Updates the mapping artifact with "Auto-Patches" if it finds obvious missing fields (e.g., `Remaks` -> `metadata.remarks`). These patches are immediately available to the Importer and Learner.

4.  **Importer (Step 4)**
    *   **Action**: bulk transforms and saves all data.
    *   **Logic**: Applies the mapping artifact (including Auditor patches) to ALL rows in the `RawRow` table and upserts `Container`, `Shipment`, and `ContainerEvent` records. Preserves `temp_translation.json` for Step 5 analysis.
    *   **Output**: Live database records.

5.  **Learner (Step 5)**
    *   **Action**: Learns from unmapped data to improve future runs.
    *   **Action**: Learns from unmapped data to improve future runs.
    *   **Logic**: 
        1.  Scans the imported containers for "Unmapped Fields" (stored in metadata) and uses the `ImprovementAnalyzer` AI.
        2.  **Reads the `temp_translation.json` artifact** to capture successful "Auto-Patches" generated by the Auditor in Step 3.
    *   **Output**: Updates `agents/dictionaries/business_units.yml` or `container_ontology.yml` automatically, ensuring the system learns from its own corrections.

## Logging Infrastructure

To verify runs and troubleshoot issues without relying on the UI console, we have implemented a robust file-based logging system.

*   **Location**: `logs/simulation.log` (Project Root)
*   **Behavior**:
    *   Real-time logs are appended here during every simulation step.
    *   **Auto-Archiving**: When a new simulation starts, the previous log is renamed to `logs/simulation_<timestamp>.log`.
*   **Usage**:
    *   Tail this file to see Agent thoughts, API responses, and specific errors.
    *   Example: `cat logs/simulation.log`

## Current Issues & Troubleshooting

### 1. Auditor "Stuck" State (Resolved)
**Symptoms**: The simulation hangs during Step 3 (Auditor). The logs show 2-3 containers processed, then silence. The test script times out after 60 seconds.
**Cause**: The `Auditor` agent makes sequential, heavy AI calls (Azure OpenAI `gpt-4o`). If the API is slow or rate-limited, the strict timeout in the loop halts the process, or it simply takes longer than the test script allows.
**Resolution**:
*   Reduced the sample size in Step 3 (from 5 to 3) in `scripts/step3_auditor.ts`.
*   Added a 15-second strict timeout wrapper to the AI calls in `agents/auditor.ts` and wrapped the call in a try/catch block to skip failures gracefully.
*   The simulation now successfully completes even if the auditor times out on specific containers.

### 2. "Node.exe" Popups (Resolved)
**Symptoms**: Infinite loop of command prompt windows opening/closing.
**Cause**: Race condition between Frontend `status` polling and Backend `spawn`.
**Fix**: `app/api/simulation/control/route.ts` now updates status *synchronously* before spawning child processes. Step scripts use `shell: false` and `windowsHide: true`.

### 3. Step 4 "Null" Data (Resolved)
**Symptoms**: Imported containers showed `null` for all fields.
**Cause**: `transformRow` utility expected Array input but received Key-Value Objects.
**Fix**: Updated `lib/transformation-engine.ts` to handle both formats.

## Environment Compatibility

### Vercel / Serverless Deployments
The **Simulation Engine** (`/simulation`) is architected for **local demonstration only**. It relies on:
1.  **Persistent Filesystem**: To store state (`simulation_status.json`), log archives (`logs/`), and temporary artifacts (`temp_translation.json`) between steps.
2.  **Child Processes**: To spawn long-running TypeScript scripts (`step1_...ts`) that execute the heavy AI agent logic independently of the HTTP request cycle.

These features are **incompatible** with Vercel's serverless environment (Read-Only filesystem, no background process persistence).

**Behavior on Vercel**:
*   The "Start Simulation" and "Upload File" endpoints (`api/simulation/control`, `api/upload`) are **protected**.
*   They will return a `200 OK` with `success: false` and a descriptive message.
*   The UI will display an alert: *"Simulation Engine is meant for local demonstration only..."*
*   **Operational Dashboard**: The rest of the application (Dashboard, Container Details, Search) functions normally on Vercel using the production database.
